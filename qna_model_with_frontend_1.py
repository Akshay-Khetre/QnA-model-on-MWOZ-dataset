# -*- coding: utf-8 -*-
"""QnA Model with Frontend 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/182uoYV24qG34L3HoC3-yydegx2G__jtk
"""

pip install transformers

import pandas as pd
import torch.nn as nn
import torch
from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering, AdamW
from torch.utils.data import DataLoader, Dataset
from flask import Flask, render_template, request
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB

class QADataset(Dataset):
    def __init__(self, data, tokenizer, max_length=512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        question = self.data.loc[idx, "question"]
        context = self.data.loc[idx, "answer"]  # Use "answer" column as context for simplicity

        # Tokenize the question and context
        inputs = self.tokenizer(question, context, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')

        # Find the [SEP] token index in the tokenized input_ids
        sep_token_idx = (inputs['input_ids'] == self.tokenizer.sep_token_id).nonzero(as_tuple=False)

        # Set the start and end positions based on the [SEP] token index
        if sep_token_idx.numel() > 0:
            start_positions = torch.tensor([sep_token_idx[0, 0] + 1], dtype=torch.long)
            end_positions = torch.tensor([sep_token_idx[-1, 0] - 1], dtype=torch.long)
        else:
            # Set start and end positions to 0 if the [SEP] token is not found
            start_positions = torch.tensor([0], dtype=torch.long)
            end_positions = torch.tensor([0], dtype=torch.long)

        return {
            'input_ids': inputs['input_ids'].squeeze(),
            'attention_mask': inputs['attention_mask'].squeeze(),
            'start_positions': start_positions,
            'end_positions': end_positions
        }

def compute_exact_match(predictions, references):
    exact_match = sum(1 for p, r in zip(predictions, references) if p == r)
    total = len(predictions)
    return exact_match / total

#Load and prepare the data
file_path = '/content/mwoz_data_train.csv'
data = pd.read_csv(file_path)

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
dataset = QADataset(data, tokenizer)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)

loss_function = nn.CrossEntropyLoss()

train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

epochs = 1

for epoch in range(epochs):
    model.train()
    total_loss = 0
    total_exact_match = 0
    for batch in train_dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        start_positions = batch['start_positions'].to(device)
        end_positions = batch['end_positions'].to(device)
        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)
        loss = outputs.loss
        total_loss += loss.item()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Compute and accumulate exact match accuracy
        with torch.no_grad():
            start_logits = outputs.start_logits.argmax(dim=1)
            end_logits = outputs.end_logits.argmax(dim=1)
            for i in range(len(batch['input_ids'])):
                start_idx = start_logits[i].item()
                end_idx = end_logits[i].item()
                predicted_answer = tokenizer.decode(batch['input_ids'][i][start_idx: end_idx + 1], skip_special_tokens=True)
                true_answer = tokenizer.decode(batch['input_ids'][i][batch['start_positions'][i]: batch['end_positions'][i] + 1], skip_special_tokens=True)
                total_exact_match += 1 if predicted_answer == true_answer else 0

    avg_loss = total_loss / len(train_dataloader)
    accuracy = ((total_exact_match / len(dataset)))

    print(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}")

output_dir = 'distilbert_qa_model'
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

# Load the test and validation datasets
test_df = pd.read_csv("/content/mwoz_test_df.csv")
validation_df = pd.read_csv("/content/mwoz_val_df.csv")

# Calculate accuracy for the validation dataset
validation_accuracy = accuracy_score(validation_df["answer"], validation_df["Predicted answer"])

# Calculate accuracy for the test dataset
test_accuracy = accuracy_score(test_df["answer"], test_df["predicted answer"])

print(f"Validation Accuracy: {validation_accuracy * 100:.2f}%")
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# Load the CSV file into a DataFrame
df = pd.read_csv('/content/mwoz_data_qna_scores.csv')

# Assuming that 'actual answer' and 'predicted answer' contain the actual and predicted answers, respectively.
actual_answers = df['answer']
predicted_answers = df['predicted answer']

# Split the answers into lists of entities (e.g., using whitespace as a separator)
actual_entities = [answer.split() for answer in actual_answers]
predicted_entities = [answer.split() for answer in predicted_answers]

# Initialize lists to store true positives, false positives, and false negatives
true_positives = 0
false_positives = 0
false_negatives = 0

# Calculate precision, recall, and F1-score
for actual, predicted in zip(actual_entities, predicted_entities):
    for entity in predicted:
        if entity in actual:
            true_positives += 1
        else:
            false_positives += 1
    for entity in actual:
        if entity not in predicted:
            false_negatives += 1

precision = true_positives / (true_positives + false_positives)
recall = true_positives / (true_positives + false_negatives)
f1 = 2 * (precision * recall) / (precision + recall)

print("Precision: {:.2f}".format(precision))
print("Recall: {:.2f}".format(recall))
print("F1 Score: {:.2f}".format(f1))

def qa_df(model_path):
    tokenizer = DistilBertTokenizer.from_pretrained('/content/distilbert_qa_model')
    model = DistilBertForQuestionAnswering.from_pretrained('/content/distilbert_qa_model')
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    return model, tokenizer, device

app = Flask(__name__)

def load_qa_data(csv_file):
    return pd.read_csv('/content/sample_data/mwoz_data_qna.csv')

def find_answer(question, qa_df):
    question = question.lower()
    matched_row = qa_df[qa_df['question'].apply(lambda x: x.lower()) == question]
    if len(matched_row) > 0:
        return matched_row.iloc[0]['answer']
    else:
        return "Answer not found for the given question."

@app.route('/', methods=['GET', 'POST'])
def index():
    qa_df = load_qa_data('mwoz_data_qna.csv')

    if request.method == 'POST':
        user_question = request.form['question']
        answer = find_answer(user_question, qa_df)
    else:
        user_question = ""
        answer = ""

    return render_template('index.html', user_question=user_question, answer=answer)

if __name__ == '__main__':
    app.run(debug=True)